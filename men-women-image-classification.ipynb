{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the task\n",
    "\n",
    "We have 1418 and 1912 images of men and women respectively. Our task is to create a deep learning classification algorithm under different optimizers and paramters using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data']\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Data Preperation: We will make data and labels list where data will be image to array implementatation which contains RGB values of each image. and label will be class of cells here I will be taking 0 and 1 for two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "data=[]\n",
    "labels=[]\n",
    "men=os.listdir(\"../input/data/men/\")\n",
    "dummy = 0\n",
    "for a in men:\n",
    "    try:\n",
    "        image=cv2.imread(\"../input/data/men/\"+a)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((200, 200))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(0)\n",
    "    except AttributeError:\n",
    "        dummy += 1\n",
    "women=os.listdir(\"../input/data/women/\")\n",
    "for b in women:\n",
    "    try:\n",
    "        image=cv2.imread(\"../input/data/women/\"+b)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((200, 200))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(1)\n",
    "    except AttributeError:\n",
    "        dummy += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3311, 200, 200, 3)\n",
      "(3311,)\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "Cells=np.array(data)\n",
    "labels=np.array(labels)\n",
    "print(Cells.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning of Cells.shape and labels.shape\n",
    "\n",
    "Cells.shape has 3311 data points (ie images). Each data point, is represented by three 200 by 200 matrices, because we have one matrix for each RGB colour.\n",
    "\n",
    "labels.shape has 3311 labels for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "np.save(\"Cells\",Cells)\n",
    "np.save(\"labels\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "Cells=np.load(\"Cells.npy\")\n",
    "labels=np.load(\"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "s=np.arange(Cells.shape[0])\n",
    "np.random.shuffle(s)\n",
    "Cells=Cells[s]\n",
    "labels=labels[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3311\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "num_classes=len(np.unique(labels))\n",
    "len_data=len(Cells)\n",
    "\n",
    "print(num_classes)\n",
    "print(len_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning of num_classes and len_data\n",
    "\n",
    "num_classes is the number of classes, len_data is the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "(x_train,x_test)=Cells[(int)(0.1*len_data):],Cells[:(int)(0.1*len_data)]\n",
    "x_train = x_train.astype('float32')/255 # As we are working on image data we are normalizing data by dividing 255.\n",
    "x_test = x_test.astype('float32')/255\n",
    "train_len=len(x_train)\n",
    "test_len=len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2980, 200, 200, 3)\n",
      "(2980,)\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "(y_train,y_test)=labels[(int)(0.1*len_data):],labels[:(int)(0.1*len_data)]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning of x_train.shape and y_train.shape\n",
    "\n",
    "x_train has the data for 2980 images and y_train has the labels for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Keras Sequential\n",
    "## (with different optimizers)\n",
    "\n",
    "We will use the categorical cross entropy as the loss function.\n",
    "The categorical cross entropy (aka log-loss) penalizes the accuracy of a probability prediction.\n",
    "For example, if an algorithm predicts a probability of 0.6 for an image with label 1, the penalty for this prediction will be log(.6). If the label was 0, the penalty will be log(0.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 200, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               20000500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 20,012,046\n",
      "Trainable params: 20,012,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "#creating sequential model\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(200,200,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive meaning of the sequential model defined above\n",
    "\n",
    "There are three 2-dimensional convolutional filters.\n",
    "The filter sizes are (in order) 16, 32 and 64.\n",
    "For example, this means the last convolutional filter reduces the input layer more substantially than the second filter because the neuron looks at 32 pixels at once on the second filter but 64 pixels on the last filter.\n",
    "\n",
    "Each 2-dimensional convolutional filter is followed by a max pooling operation for 2-dimensions.\n",
    "The max pooling operation performs dimension reduction on the output layer from the previous convolutional filter before it becomes the output layer for the next convolutional filter.\n",
    "\n",
    "Dropout's purpose is to avoid overfitting. It aims to achieve this by randomly selecting elements from the matrix and replacing them with 0.\n",
    "\n",
    "Flatten means transforming the feature matrix into a vector.\n",
    "\n",
    "Dense represents a densley-connected neural network layer which performs a linear operation on a vector.\n",
    "The first dc-nn layer's output vector is 500-dimensional and the second is 2-dimensional.\n",
    "The two activations for Dense used above are relu and softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer with default parameters\n",
    "\n",
    "Objective: \n",
    "Minimize the loss function with parameter theta.\n",
    "\n",
    "Parameters: \n",
    "Step size denoted by alpha.\n",
    "Decay rate for first moment estimate denoted by beta1.\n",
    "Decay rate for second raw moment estimate denoted by beta2.\n",
    "Initial paremeter vector for theta. \n",
    "\n",
    "Intuitive meaning of algorithm steps: \n",
    "\n",
    "(1) Get the gradients of the loss function under the parameter vector for theta. \n",
    "\n",
    "(2) Update the first moment estimates by: beta1 x previous iteration moment estimates + (1-beta1) x gradients of loss function. \n",
    "\n",
    "(3) Update the second raw moment estimates by: beta1 x previous iteration moment estimates + (1-beta1) x squared gradients of loss function. \n",
    "Notice the larger beta is, the more bias and less variant the estimates above will be and vice-versa.\n",
    "\n",
    "(4) Bias-correct the first moment estimates by: current estimate / (1-beta1). \n",
    "\n",
    "(5) Bias correct the second raw moment estimates by: current estimate / (1-beta2). \n",
    "Notice the larger beta is, the larger the bias-corrected estimates will be and thus moving further away from convergence; vice-versa.\n",
    "\n",
    "(6) Update the parameter vector by: previous parameter vector - alpha x first moment estimates / sqrt of second moment estimates.\n",
    "\n",
    "Iterate until the updated parameter vector are essentially equal to the previous iteration's parameter vector.\n",
    "\n",
    "Reference (Kingma&Ba, 2015): https://arxiv.org/pdf/1412.6980v8.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "#Doing One hot encoding as classifier has multiple classes\n",
    "y_train=keras.utils.to_categorical(y_train,2)\n",
    "y_test=keras.utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source code: https://www.kaggle.com/tjm2018/starter-men-women-classification-e6719600-3\n",
    "#compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "2980/2980 [==============================] - 6s 2ms/step - loss: 0.7807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f090534b128>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980/2980 [==============================] - 1s 424us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6417159494137604"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for Adam Optimizer default:\n",
    "\n",
    "It's empirical loss (as shown on step 2's output) is much greater than the true loss (as shown on step 3's output). This raises sucipicioun of an underfit. Therefore we will re-train the model with smaller step sizes (i.e. decrease alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer with small alpha\n",
    "\n",
    "The default aplpha value is number of samples in dataset divided by 32. We will change this denominator to 64 to halve the alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2980/2980 [==============================] - 2s 754us/step - loss: 0.6253\n",
      "2980/2980 [==============================] - 1s 395us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6047906748400439"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 64)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success. We realise the empirical loss is still larger than the true loss but the discrepancy has decreased significantly. More importantly, the true loss has decreased. Now we will observe the Adamax Optimizer which was introduced by the same paper that introduced Adam Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adamax Optimizer\n",
    "\n",
    "Objective: \n",
    "Minimize the loss function with parameter theta.\n",
    "\n",
    "Parameters: \n",
    "Step size denoted by alpha.\n",
    "Decay rate for first moment estimate denoted by beta1.\n",
    "Decay rate for exponentially wieghted inifinity norm denoted by beta2.\n",
    "Initial paremeter vector for theta. \n",
    "\n",
    "Intuitive meaning of algorithm steps: \n",
    "\n",
    "(1) Get the gradients of the loss function under the parameter vector for theta.\n",
    "\n",
    "(2) Update the first moment vector by: beta1 x current first moment vector + (1 - beta1) x gradients of the loss function\n",
    "\n",
    "(3) Update the exponentially wieghted inifinity norm by: max( b2 x exponentially wieghted inifinity norm, absolute value gradients of the loss function )\n",
    "\n",
    "(4) Update parameter vector by: current parameter vector - (alpha / (1 - beta1^iteration_count) ) * first moment vector / exponentially wieghted inifinity norm\n",
    "\n",
    "Iterate until the updated parameter vector are essentially equal to the previous iteration's parameter vector.\n",
    "\n",
    "Reference (Kingma&Ba, 2015): https://arxiv.org/pdf/1412.6980v8.pdf\n",
    "\n",
    "Now we will fit this optimizer for our dataset with default parameters and then by halving alpha like we did for observing Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2980/2980 [==============================] - 4s 1ms/step - loss: 0.5964\n",
      "2980/2980 [==============================] - 1s 411us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5420882996296723"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adamax')\n",
    "model.fit(x_train, y_train)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2980/2980 [==============================] - 3s 1ms/step - loss: 0.5448\n",
      "2980/2980 [==============================] - 1s 428us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48027771699348554"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adamax')\n",
    "model.fit(x_train, y_train, batch_size = 64)\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Adamax, we notice the discrepancy between the true loss and empirical loss doesn't change a lot for the different alpha values, which is unlike what we observed for Adam. However, again, we notice that the true loss has decreased after alpha was halved.\n",
    "\n",
    "When we compare the performance of Adam and Adamax for this dataset, Adam seems to perform better in terms of over/under fitting. This supports what was mentioned on the paper, how Adamax suits loss functions L3-norm or higher. Our loss function, categorical crossentropy function, is log L1-norm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
